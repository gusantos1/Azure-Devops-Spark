{"cells":[{"cell_type":"markdown","source":["# Azure-Devops-Spark\n> A productive library to extract data from Azure Devops and apply agile metrics.\n\nPypi.org: https://pypi.org/project/Azure-Devops-Spark/  \ngithub: https://github.com/gusantos1/Azure-Devops-Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4122c836-2f00-47e3-9c98-e7fb2355714b"}}},{"cell_type":"markdown","source":["## Install Package"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adb34f3f-1a70-4fe5-9380-d68b7636cfcb"}}},{"cell_type":"code","source":["pip install Azure-Devops-Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c63a763-9f32-494b-a958-83a918ad6804"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Import"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bb44354-b635-4ce5-a252-b188edb4e188"}}},{"cell_type":"code","source":["from AzureDevopsSpark import Azure, Agile\nfrom pyspark.sql.functions import datediff\nfrom operator import truediv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e40fe8b-b1a7-405b-bd42-f126432687c6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Azure"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5cf37c2-512a-4c17-88c4-c0d6da1f4ff9"}}},{"cell_type":"code","source":["devops = Azure('organization', 'project', 'token')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9dcec72-147a-478b-8701-67f953e9c417"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Members"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98675163-aeb5-4d8b-b870-f20710ab23a6"}}},{"cell_type":"code","source":["members = devops.all_members().df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae7b1447-bbe3-4068-816b-d9ba480533b4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Squads"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b6316f9-f650-4b60-87e8-930bbb0dd903"}}},{"cell_type":"code","source":["squads = devops.all_teams().df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d91178a1-720f-49f0-afb3-3beee7d2cd75"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Iterations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16561ec3-4611-4b9b-8791-5332dadacf4a"}}},{"cell_type":"code","source":["iterations = devops.all_iterations().df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ee33a62-9c7b-4714-837b-22eef2dd41d4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Items"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f781744-9d8e-4b9a-9573-f9481a9b8936"}}},{"cell_type":"code","source":["df_items = devops.all_items().df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b5b6847-8995-4a22-8d08-7e0341aea678"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Backlog"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddb45a13-aa1b-47ab-bfa5-ab462a6f3334"}}},{"cell_type":"code","source":["df_backlog = devops.all_backlog().df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1f7f183-3b25-4654-a41b-50b5fb7c2585"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Join"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91e6ed91-db0d-4c69-afe3-9ae93503e8c1"}}},{"cell_type":"code","source":["full = df_items.join(iterations, df_items.IterationPath == iterations.Iteration_Path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1a86d00-28f6-4c8b-ae38-2e8051c9f275"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Base Agile"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72b77ae9-97ac-4cdc-82e8-2aa8de3accbb"}}},{"cell_type":"code","source":["df_agil = full.select(\n    'AreaPath', 'IterationPath', 'Iteration_Start_Date', 'Iteration_End_Date', 'WorkItemType', 'Id', 'AssignedTo', 'CreatedDate', 'ClosedDate', 'ChangedDate', 'ActivatedDate', 'State', 'Effort')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ab6695c-556e-42b4-9081-98b8568b6123"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Agile Metrics\n> The Agile class receives any PySpark dataframe, it is formed by aggregation methods and types of filters that make customization flexible to apply agile metrics. Agile doesn't have, for example, a cycle time method, but it is possible to create from the avg method with your customizations.\n\nAll public methods of this class return a Detail object containing detail and df attributes, detail is the dataframe version before aggregation and df is the dataframe already aggregated.\n\n- avg, count, max, min, sum\n###### After filtering a dataframe, it performs the operation on the column passed as an argument in ref.\n\n```avg(self, df, ref: Union[str, list], iteration_path: str, new: str, literal_filter: List[str] = None, between_date: Dict[str, str] = None, group_by: List[str] = None, **filters)```\n\n<br/>\n\n- custom  \n###### Agile.custom takes two PySpark dataframes and the information needed to merge and apply an operation between two columns. Supported Operators: is_, is_not, add, and_, floordiv, mod, mul, pow, sub e ceil (Pyspark).\n\n  ```python custom(self, df_left, def_right, left: str, right: str, how: str, op: operator, left_ref: str, right_ref: str, new: str)```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5398b44-6ecb-4646-b7cf-fc665e4911b8"}}},{"cell_type":"code","source":["agile = Agile()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98e920a1-fe74-4e4b-961e-3f681682e7b6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Lead Time"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e25fd66-36dc-4f8e-bc29-869a8090ebd5"}}},{"cell_type":"code","source":["df_lead_time = agile.avg(\n    df=df_agil,\n    ref=[datediff, 'ClosedDate', 'CreatedDate'], # You can pass the signature of the datediff method as a parameter which will result in the ClosedDate - CreatedDate operation.\n    iteration_path='IterationPath',\n    new='LeadTimePbiDaysIn90Days',\n    literal_filter=['ClosedDate >= 90'], # Agile knows that 'ClosedDate' is a DateType instance, so ClosedDate >= (D-90).\n    filters={\n        'WorkItemType': 'Product Backlog Item',\n        'State': 'Done'}\n).df\n\n# We could also filter between a time range between 2022-01-01 to 2022-12-31 with between_date.\n\n#     between_date = {\n#         'CreatedDate': '2022-01-01',\n#         'ClosedDate': '2022-12-31'\n#     },    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f60b664d-d952-4b40-bcbd-58e6751f87c6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Backlog"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13a63c49-473b-4b63-a83f-7342bfaefbdf"}}},{"cell_type":"code","source":["df_qtd_backlog = agile.count(\n  df=df_backlog,\n  ref='Id',\n  iteration_path='IterationPath',\n  new='QtdBacklog',\n  filters={\n      'WorkItemType': ['Product Backlog Item', 'Improvement', 'Bug', 'Issue', 'Technical Debt', 'Spike', 'Vulnerability'],\n      'State': '<>Removed'} # Use <> for negation logical expressions.\n).df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42b2f8de-0149-433e-b286-da69cc134573"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Backlog (Bug + Technical Debt) / Count(Backlog)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7de75fb1-34e4-4e6b-983e-31cb2ac122f9"}}},{"cell_type":"code","source":["df_qtd_bug_debt = agile.count(\n    df=df_backlog,\n    ref='Id',\n    iteration_path='IterationPath',\n    new='QtdBugDebt',\n    filters={'WorkItemType': ['Bug', 'Technical Debt']}\n).df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1aa9261e-5417-4085-93c2-288325fe77d0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Without using the custom method"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f719dadc-4ffc-4e15-893c-57fd0113fa01"}}},{"cell_type":"code","source":["temp = df_qtd_bug_debt.join(df_qtd_backlog, df_qtd_bug_debt['IterationPath'] == df_qtd_backlog['IterationPath'])\ntemp_two = temp.withColumn('BacklogHealthCalc', temp['QtdBugDebt'] / temp['QtdBacklog'])\n\n# df_backlog_bug_tech = temp_two.select('IterationPath', 'BacklogBugTechDebt').df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adab921c-7b42-44f8-a021-e0b74e90db0f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Using the custom method\n> Agile.custom receives two PySpark dataframes and the information needed to apply the join and the signature of a Python operator that will do the operation between the two columns."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1034ee94-141e-4d8c-8fdb-34288e167191"}}},{"cell_type":"code","source":["df_backlog_bug_tech = agile.custom(df_start_diff, df_qtd_itens, 'IterationPath', 'IterationPath', 'left', truediv, 'BacklogBugTechDebt').df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"518bee23-0176-42c3-ae7e-e10810e353e4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Multiple join"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04cb52fa-1076-4d3a-bb41-f541d3aefa06"}}},{"cell_type":"code","source":["dataframes = [\n    df_lead_time,\n    df_qtd_backlog,\n    df_backlog_bug_tech,\n    df_backlog_bug_tech\n]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3cc2e27-07fd-4233-824e-bc34935c5de6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_agile_metrics = agile.multiple_join(dataframes, 'IterationPath')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36271921-a822-4ec5-9ea4-bd162eeda890"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Author\n\nThe Azure-Devops-Spark library was written by Guilherme Silva < https://www.linkedin.com/in/gusantosdev/ > in 2022."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13b57066-1e66-4a7a-90b7-c1c6160a1f0e"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Example_Azure_Devops_Spark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4173521366157546}},"nbformat":4,"nbformat_minor":0}
