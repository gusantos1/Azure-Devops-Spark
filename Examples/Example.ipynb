{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4122c836-2f00-47e3-9c98-e7fb2355714b","showTitle":false,"title":""}},"source":["%md\n","# azure-devops-pyspark\n",">  Azure Devops PySpark: A productive library to extract data from Azure Devops and apply agile metrics.\n","\n","Pypi.org: https://pypi.org/project/azure-devops-pyspark/  \n","github: https://github.com/gusantos1/azure-devops-pyspark/"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"adb34f3f-1a70-4fe5-9380-d68b7636cfcb","showTitle":false,"title":""}},"source":["## Install Package"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4c63a763-9f32-494b-a958-83a918ad6804","showTitle":false,"title":""}},"outputs":[],"source":["pip install azure-devops-pyspark"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9bb44354-b635-4ce5-a252-b188edb4e188","showTitle":false,"title":""}},"source":["## Import"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9e40fe8b-b1a7-405b-bd42-f126432687c6","showTitle":false,"title":""}},"outputs":[],"source":["from AzureDevopsPySpark import Azure, Agile\n","from pyspark.sql.functions import datediff"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e5cf37c2-512a-4c17-88c4-c0d6da1f4ff9","showTitle":false,"title":""}},"source":["## Azure"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f9dcec72-147a-478b-8701-67f953e9c417","showTitle":false,"title":""}},"outputs":[],"source":["devops = Azure('organization', 'project', 'token')"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"98675163-aeb5-4d8b-b870-f20710ab23a6","showTitle":false,"title":""}},"source":["## Members"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ae7b1447-bbe3-4068-816b-d9ba480533b4","showTitle":false,"title":""}},"outputs":[],"source":["members = devops.all_members().df"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6b6316f9-f650-4b60-87e8-930bbb0dd903","showTitle":false,"title":""}},"source":["## Squads"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d91178a1-720f-49f0-afb3-3beee7d2cd75","showTitle":false,"title":""}},"outputs":[],"source":["squads = devops.all_teams().df"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"16561ec3-4611-4b9b-8791-5332dadacf4a","showTitle":false,"title":""}},"source":["## Iterations"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2ee33a62-9c7b-4714-837b-22eef2dd41d4","showTitle":false,"title":""}},"outputs":[],"source":["iterations = devops.all_iterations().df"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0f781744-9d8e-4b9a-9573-f9481a9b8936","showTitle":false,"title":""}},"source":["## Items"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3b5b6847-8995-4a22-8d08-7e0341aea678","showTitle":false,"title":""}},"outputs":[],"source":["df_items = devops.all_items().df"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ddb45a13-aa1b-47ab-bfa5-ab462a6f3334","showTitle":false,"title":""}},"source":["## Backlog"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b1f7f183-3b25-4654-a41b-50b5fb7c2585","showTitle":false,"title":""}},"outputs":[],"source":["df_backlog = devops.all_backlog().df"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"91e6ed91-db0d-4c69-afe3-9ae93503e8c1","showTitle":false,"title":""}},"source":["## Join"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d1a86d00-28f6-4c8b-ae38-2e8051c9f275","showTitle":false,"title":""}},"outputs":[],"source":["full = df_items.join(iterations, df_items.IterationPath == iterations.Iteration_Path)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"72b77ae9-97ac-4cdc-82e8-2aa8de3accbb","showTitle":false,"title":""}},"source":["## Base Agile"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4ab6695c-556e-42b4-9081-98b8568b6123","showTitle":false,"title":""}},"outputs":[],"source":["df_agil = full.select(\n","    'AreaPath', 'IterationPath', 'Iteration_Start_Date', 'Iteration_End_Date', 'WorkItemType', 'Id', 'AssignedTo', 'CreatedDate', 'ClosedDate', 'ChangedDate', 'ActivatedDate', 'State', 'Effort')"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e5398b44-6ecb-4646-b7cf-fc665e4911b8","showTitle":false,"title":""}},"source":["%md\n","# Agile Metrics\n","The Agile class receives any PySpark dataframe, it is formed by aggregation methods and types of filters that make customization flexible to apply agile metrics. Agile doesn't have, for example, a cycle time method, but it is possible to create from the avg method with your customizations.\n","\n","All public methods of this class return a Detail object containing detail and df attributes, detail is the dataframe version before aggregation and df is the dataframe already aggregated.\n","\n","- avg, count, max, min, sum\n","\n","  ###### After filtering a dataframe, it performs the operation on the column passed as an argument in ref.\n","\n","  ```python\n","  avg(self, df, ref: Union[str, list], iteration_path: str, new: str, literal_filter: List[str] = None, between_date: Dict[str, str] = None, group_by: List[str] = None, **filters)\n","  ```\n","\n","- custom\n","\n","  ###### Agile.custom receives two PySpark dataframes and the information needed to merge and the signature string of a Python operator that will do the operation between the two columns. Supported operators: is_, is_not, add, and_, truediv, floordiv, mod, mul, pow, sub and ceil (Pyspark).\n","\n","  ```python\n","  custom(self, df_left, def_right, left: str, right: str, how: str, op: operator, left_ref: str, right_ref: str, new: str)\n","  ```\n","  \n","- multiple_join\n","\n","  ###### Receives a list of dataframes and merges using the same column name between them.\n","\n","  ```python\n","  multiple_join(self, dfs: list, on: List[str], how: str = 'left')\n","  ```"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"98e920a1-fe74-4e4b-961e-3f681682e7b6","showTitle":false,"title":""}},"outputs":[],"source":["agile = Agile()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6e25fd66-36dc-4f8e-bc29-869a8090ebd5","showTitle":false,"title":""}},"source":["## Lead Time"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f60b664d-d952-4b40-bcbd-58e6751f87c6","showTitle":false,"title":""}},"outputs":[],"source":["df_lead_time = agile.avg(\n","    df=df_agil,\n","    ref=[datediff, 'ClosedDate', 'CreatedDate'], # You can pass the signature of the datediff method as a parameter which will result in the ClosedDate - CreatedDate operation.\n","    iteration_path='IterationPath',\n","    new='LeadTimePbiDaysIn90Days',\n","    literal_filter=['ClosedDate >= 90'], # Agile knows that 'ClosedDate' is a DateType instance, so ClosedDate >= (D-90).\n","    filters={\n","        'WorkItemType': 'Product Backlog Item',\n","        'State': 'Done'}\n",").df\n","\n","# We could also filter between a time range between 2022-01-01 to 2022-12-31 with between_date.\n","\n","#     between_date = {\n","#         'CreatedDate': '2022-01-01',\n","#         'ClosedDate': '2022-12-31'\n","#     },    "]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"13a63c49-473b-4b63-a83f-7342bfaefbdf","showTitle":false,"title":""}},"source":["## Backlog"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"42b2f8de-0149-433e-b286-da69cc134573","showTitle":false,"title":""}},"outputs":[],"source":["df_qtd_backlog = agile.count(\n","  df=df_backlog,\n","  ref='Id',\n","  iteration_path='IterationPath',\n","  new='QtdBacklog',\n","  filters={\n","      'WorkItemType': ['Product Backlog Item', 'Improvement', 'Bug', 'Issue', 'Technical Debt', 'Spike', 'Vulnerability'],\n","      'State': '<>Removed'} # Use <> for negation logical expressions.\n",").df"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7de75fb1-34e4-4e6b-983e-31cb2ac122f9","showTitle":false,"title":""}},"source":["## Backlog (Bug + Technical Debt) / Count(Backlog)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1aa9261e-5417-4085-93c2-288325fe77d0","showTitle":false,"title":""}},"outputs":[],"source":["df_qtd_bug_debt = agile.count(\n","    df=df_backlog,\n","    ref='Id',\n","    iteration_path='IterationPath',\n","    new='QtdBugDebt',\n","    filters={'WorkItemType': ['Bug', 'Technical Debt']}\n",").df"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f719dadc-4ffc-4e15-893c-57fd0113fa01","showTitle":false,"title":""}},"source":["##### Without using the custom method"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"adab921c-7b42-44f8-a021-e0b74e90db0f","showTitle":false,"title":""}},"outputs":[],"source":["temp = df_qtd_bug_debt.join(df_qtd_backlog, df_qtd_bug_debt['IterationPath'] == df_qtd_backlog['IterationPath'])\n","temp_two = temp.withColumn('BacklogHealthCalc', temp['QtdBugDebt'] / temp['QtdBacklog'])\n","\n","# df_backlog_bug_tech = temp_two.select('IterationPath', 'BacklogBugTechDebt').df"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1034ee94-141e-4d8c-8fdb-34288e167191","showTitle":false,"title":""}},"source":["##### Using the custom method\n","> Agile.custom receives two PySpark dataframes and the information needed to apply the join and the signature of a Python operator that will do the operation between the two columns."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"518bee23-0176-42c3-ae7e-e10810e353e4","showTitle":false,"title":""}},"outputs":[],"source":["df_backlog_bug_debt = agile.custom(df_left=df_qtd_bug_debt, df_right=df_qtd_backlog, left='IterationPath', right='IterationPath', how='left', op='truediv', left_ref='QtdBugDebt', right_ref='QtdBacklog', new='BacklogBugDebt').df"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"04cb52fa-1076-4d3a-bb41-f541d3aefa06","showTitle":false,"title":""}},"source":["## Multiple join"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a3cc2e27-07fd-4233-824e-bc34935c5de6","showTitle":false,"title":""}},"outputs":[],"source":["dataframes = [\n","    df_lead_time,\n","    df_qtd_backlog,\n","    df_backlog_bug_tech\n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"36271921-a822-4ec5-9ea4-bd162eeda890","showTitle":false,"title":""}},"outputs":[],"source":["df_agile_metrics = agile.multiple_join(dfs=dataframes, on='IterationPath', how='left').df"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"13b57066-1e66-4a7a-90b7-c1c6160a1f0e","showTitle":false,"title":""}},"source":["## Author\n","\n","The azure-devops-pyspark library was written by Guilherme Silva < https://www.linkedin.com/in/gusantosdev/ > in 2022."]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Example_Azure_Devops_Spark","notebookOrigID":4173521366157546,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
